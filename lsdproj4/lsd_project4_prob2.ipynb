{
 "metadata": {
  "name": "lsd_project4_prob2_main"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": "Part 2 & 3 of project 4, done by Zi Chong Kao and Sen Tian"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "import DAL\n\nfrom IPython.parallel import Client\nrc = Client()\ndview = rc[:]\nprint rc.ids",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n"
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# import modules on all engines\nwith dview.sync_imports():\n    from DAL.datasets.checkpoint import Checkpoint\n    import numpy\n    from numpy import float32\n    ",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "importing Checkpoint from DAL.datasets.checkpoint on engine(s)\nimporting numpy on engine(s)\nimporting float32 from numpy on engine(s)\n"
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# function for removing pairs of elements from numpy arrays that are Nan\ndef delete_nan(list_1, list_2):\n    temp_list = numpy.array([list_1, list_2], dtype=float32).transpose()\n    return temp_list[~numpy.isnan(temp_list).any(1)].transpose()\n\n# function of Gaussian Kernel\ndef gaussian_kernel(matrix_x):\n    return numpy.exp( - matrix_x ** 2 / 2.0 )\n\n# function of Epanechnikov kernel, here I put 'import numpy' twice because it is required when we try to do parallelization and deal with closures\ndef epanech_kernel(matrix_x):\n    matrix_without_indicator = 1 - matrix_x ** 2\n    # set negative entries to zero\n    matrix_with_indicator = matrix_without_indicator.clip(min=0)\n    return matrix_with_indicator\n\n# function of Boxcar kernel\ndef boxcar_kernel(matrix_x):\n    matrix_with_indicator = numpy.absolute(matrix_x)<=1\n    return matrix_with_indicator \n    # careful - this returns true/false matrix. \n    # coerced into int later on\n    \n# function of Tricube kernel\ndef tricube_kernel(matrix_x):\n    matrix_without_indicator = 1 - np.absolute(matrix_x ** 3)\n    # set negative entries to zero\n    matrix_with_indicator = matrix_without_indicator.clip(min=0)\n    return matrix_with_indicator\n\ndview['delete_nan'] = delete_nan\ndview['gaussian_kernel'] = gaussian_kernel\ndview['epanech_kernel'] = epanech_kernel\ndview['boxcar_kernel'] = boxcar_kernel\ndview['tricube_kernel'] = tricube_kernel",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# function of forming matrix L\ndef matrix_L(x, bandwidth):\n    \n    # initialization of a 2-d matrix\n    diff_matrix = numpy.zeros(len(x)**2, dtype = float32).reshape((len(x), len(x))) # initialization\n    # substract each element of covariate x (vector) by its i-th element x_i, as the ith row of diff_matrix\n    for i in xrange(len(x)):\n        diff_matrix[i] = x - x[i]\n        \n    # divide each entry of diff_matrix by the bandwidth (corresponding to x_i-x_j / bandwidth)\n    diff_matrix = diff_matrix / float32(bandwidth)\n    \n    # choose a kernel function to obtain the Kernel matrix\n    L_matrix  = epanech_kernel(diff_matrix)\n\n    # vector as the sum over each row of Kernel matrix\n    sum_row = numpy.dot(L_matrix, numpy.ones(len(x), dtype=float32))\n    # divide each element in the Kernel matrix by corresponding sum to get the L matrix\n    L_matrix = numpy.dot(numpy.diag( 1.0 / sum_row ) , L_matrix)\n\n    return L_matrix\n\ndview['matrix_L'] = matrix_L",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# lightcurve data\n#clear the cache of any inconsistencies; important for handling crashes\nDAL.cleancache()\nlightcurves = DAL.create('lightcurves')\ns = lightcurves.subsets() # s is a list of zip files\n\ndview['lightcurves'] = lightcurves\ndview['s'] = s",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# use a dictionary to store the norm of thresholded residual\n\n# main function\ndef rank_star(m):\n    \n    norm = {}\n    checkpoint = Checkpoint()\n    \n    for i in lightcurves.iter(s[m]):\n        # id of the lightcurve\n        name = i['id']\n        # time and flux of this lightcurve\n        lc = i['data']\n        half = len(lc)/2\n        time = lc[:half]  #modified - first half time\n        flux = lc[half:] #modified - first half flux\n        # delete both time and flux if NAN appears in either of them\n        time_flux = delete_nan(time, flux)\n            \n        # Step one: choose bandwidth by minimizing the leave-one-out cross-validation risk\n\n        # response (vector)\n        Y = time_flux[1]\n        # covariate (vector)\n        X = time_flux[0]\n        n = len(X)\n        # give a big enough threshold for choosing minimum risk\n        risk_thres = 1000000000\n        # cross-validation (3 values of bandwidth)\n        for h in [0.1, 0.5, 1.0, 1.5]:\n            # get the L matrix\n            L = matrix_L(X, h)\n            # estimation\n            Y_hat = numpy.dot(L, Y)\n            # residual\n            resi = Y - Y_hat\n            # LOO risk\n            prod_matrix = numpy.diag(1.0 / (numpy.ones(n, dtype=float32) - numpy.diagonal(L)))\n            prod_vector = numpy.dot(prod_matrix, resi)\n            LOO_risk = numpy.sum(prod_vector ** 2) / float32(n)\n            # find out bandwidth giving the minimum LOO risk\n            if (LOO_risk < risk_thres):\n                risk_thres = LOO_risk\n                # Residual after cross validation (corresponds to step two in the algorithm)\n                resi_final = resi\n            norm[name]=resi_final\n            \n        checkpoint.store(m,obj=norm)\n    return 0",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# overwrote the first run's results\n# utility loop for checking which sets still left to be processed by the second run\ncheckpoint = Checkpoint()\nfor i in range(126,162):\n    resi_dict = checkpoint.load(i,t=\"obj\")\n    try:\n        for k,v in resi_dict.iteritems():\n            #print len(v),i\n            break\n    except:\n        print \"no\",i",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# running\nstart = datetime.datetime.now()\ntodo = range(126,162)\ndview.map_sync(rank_star, [303])\nprint datetime.datetime.now() - start",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Scoring"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# complete step three and four: standardizing the thresholding residuals \n# then load everything into dictionary\nstorage = {}\ncheckpoint = Checkpoint()\nfor i in [303]:\n    \n    resi_dict = checkpoint.load(i,t=\"obj\")\n    \n    for name,resi_final in resi_dict.iteritems():\n        n = len(resi_dict)\n        beta = numpy.sqrt(2 * numpy.log(n))\n        \n        # Step three: standardize the residual\n        MAD = numpy.median(numpy.absolute(resi_final-numpy.median(resi_final)))\n        r = (resi_final - numpy.mean(resi_final)) / (1.4826 * MAD)\n        \n        # Step four: threshold the residual\n        r[r>=-beta]=0\n        # Step five: compute the l1 norm of thresholded residual (store in a dictionary)\n        storage[name] = numpy.sum(numpy.absolute(r))",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "'numpy.float64' object does not support item assignment",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-44-1bbd6a132f3e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m# Step four: threshold the residual\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[1;31m# Step five: compute the l1 norm of thresholded residual (store in a dictionary)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mstorage\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabsolute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mTypeError\u001b[0m: 'numpy.float64' object does not support item assignment"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": "-c:15: RuntimeWarning: invalid value encountered in double_scalars\n"
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# write dict to a text file\nwith open(\"lightcurve_prob2_secondhalf.txt\",\"wb\") as f:\n    for k,v in storage.iteritems():\n        string = str(k)+\" \"+str(v)+\"\\n\"\n        f.write(string)\n# combine secondhalf with first half from sentian",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# retrieve full text file, store in dict\nstorage = {}\nwith open(\"lightcurve_prob2_full.txt\",\"rb\") as g:\n    for text in g.readlines():\n        id_norm = text.split()\n        storage[int(id_norm[0])]=float(id_norm[1])",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# sort dict in preparation for scoring\nimport operator\nsortedlist = sorted(storage.iteritems(),key=operator.itemgetter(1),reverse=True)\n# print sortedlist[:10]\nranking = [x[0] for x in sortedlist]\n# print ranking[:10]",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# scoring. warning, can only run 10 times.\nlightcurves = DAL.create('lightcurves')\nscores = lightcurves.score(ranking) #ranking is a list of ids (order matters!)\nprint scores",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "For boxcar, h = {0.1,0.5,1.0}: {u'score': 0.3817799687385559, u'score_with_candidates': 0.5003082752227783}\n"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "For epanech, h = {0.1,0.5,1.0,1.5}::{u'score': 0.3255731165409088, u'score_with_candidates': 0.4277932941913605} But this was with 142k stars rated only. Didnt have time to run the rest. We submitted the results for the boxcar."
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": "Part 3"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "The strategy we adopted was to run along each light curve, and identify peaks by noting when the values exceed and then return to the beta threshold. We calculate the height of the peak, or in this case depth of the valley, and divide it by the period of the peak to obtain a measure of the \"sharpness\" of the peak. We expect that planet transitions give sharp peaks, while eclipsing binaries give broad peaks. "
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# takes in a list of residuals r\n# output: for each peak, (period of the peak, values of the peak)\ndef peakscore(r):\n    \n    peakstore = []\n    \n    # initialize \n    peak = False\n    peaktime = 0\n    peakvalues = []\n    n = len(r)\n    beta = numpy.sqrt(2 * numpy.log(n))\n    \n    for i in r:\n        # on peak\n        if peak:    \n            if i<-beta: # still on peak\n                peaktime += 1\n                peakvalues.append(i)\n            else:       # encountered end of peak\n                peakstore.append(min(peakvalues)/float32(peaktime)) # output results of peak\n                # print peaktime, min(peakvalues)\n                peak = False  # reset peak indicator\n                peaktime = 0  # reset peak timer\n                peakvalues = []\n        # on flat portion\n        else:\n            if i<-beta: # encountered start of peak\n                peak = True\n                peaktime += 1\n                peakvalues.append(i)\n            else:\n                pass # no peak found yet\n\n    return -numpy.mean(peakstore)\n\ndview['peakscore']=peakscore",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# test peakscore on individual stars\nsetid = 299\nstarid = 6258272\nresi_dict = checkpoint.load(setid, t=\"obj\")\nresi = resi_dict[starid]\nstart = datetime.datetime.now()\nprint peakscore(resi)\nprint datetime.datetime.now() - start",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "-8.38557819271\n0:00:00.022838\n"
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# conf (a) scores 35\n# conf (c) scores 46\n# eb (a) scores 199 (not good)\n# eb (c) scores 10\n# fp (b) scores 5\n# fp (d) scores 11\n# fp (e) scores 8",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# function for parallelizing peakscore out onto clusters to operate on sets\ndef score_resids(m):\n    checkpoint = Checkpoint()\n    resi_dict = checkpoint.load(m,t=\"obj\")\n    result_list = []\n    for k,v in resi_dict.iteritems():\n        result = (k,peakscore(v))\n        result_list.append(result)\n    return result_list",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "results = dview.map_sync(score_resids,(range(126,303)+range(304,338)))",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# 303 had some problems, so I fixed it and then added in scores separately.\nresults2 = score_resids(303)",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "results3 = results+results2",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "with open(\"results1.txt\",\"rb\") as g:\n    for line in g.readline():\n        array = line.split()\n        results3.append((array[0],array[1]))",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "with open(\"final_result.txt\",\"wb\") as h:\n    for (k,v) in sorted(results3, key = lambda x: x[1], reverse=True):\n        string = str(k)+\" \"+str(v)+\"\\n\"\n        f.write(string)",
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}